{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentimental-Analysis-Using-RNN.ipynb",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "itAw4tiVKU8P",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Sentiment Analysis:** the process of computationally identifying and categorizing opinions expressed in a piece of text, especially in order to determine whether the writer's attitude towards a particular topic, product, etc. is positive, negative, or neutral."
      ]
    },
    {
      "metadata": {
        "id": "1hX9sromKU8S",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "As an improvement to my previous [Kernel][1], here I am trying to achieve better results with a Recurrent Neural Network."
      ]
    },
    {
      "metadata": {
        "id": "CsEbglUQKU8U",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "8f140ae6-066b-411f-ec61-02ad5075b8e2",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1530236611141,
          "user_tz": -330,
          "elapsed": 890,
          "user": {
            "displayName": "Abraar Syed",
            "photoUrl": "//lh5.googleusercontent.com/-ocaYyjrX3Z4/AAAAAAAAAAI/AAAAAAAAABQ/cPl8U19wmMg/s50-c-k-no/photo.jpg",
            "userId": "115904709554351828058"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load in \n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, LSTM\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.utils.np_utils import to_categorical\n",
        "import re\n",
        "\n",
        "# Input data files are available in the \"../input/\" directory.\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KUn6iO8DKU8c",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Only keeping the necessary columns."
      ]
    },
    {
      "metadata": {
        "id": "e3mwQQ5JKU8d",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "08d4c346-e035-4651-aa7a-a2681420a335",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1530236614254,
          "user_tz": -330,
          "elapsed": 3010,
          "user": {
            "displayName": "Abraar Syed",
            "photoUrl": "//lh5.googleusercontent.com/-ocaYyjrX3Z4/AAAAAAAAAAI/AAAAAAAAABQ/cPl8U19wmMg/s50-c-k-no/photo.jpg",
            "userId": "115904709554351828058"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!wget https://play.minio.io:9000/rao/Sentiment.csv -P /tmp\n",
        "data = pd.read_csv('/tmp/Sentiment.csv')\n",
        "# Keeping only the neccessary columns\n",
        "data = data[['text','sentiment']]"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2018-06-29 01:43:32--  https://play.minio.io:9000/rao/Sentiment.csv\r\n",
            "Resolving play.minio.io (play.minio.io)... 147.75.201.93\n",
            "Connecting to play.minio.io (play.minio.io)|147.75.201.93|:9000... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3967244 (3.8M) [text/csv]\n",
            "Saving to: ‘/tmp/Sentiment.csv.1’\n",
            "\n",
            "Sentiment.csv.1     100%[===================>]   3.78M  6.74MB/s    in 0.6s    \n",
            "\n",
            "2018-06-29 01:43:33 (6.74 MB/s) - ‘/tmp/Sentiment.csv.1’ saved [3967244/3967244]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "VjiIbgcrKU8g",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Next, I am dropping the 'Neutral' sentiments as my goal was to only differentiate positive and negative tweets. After that, I am filtering the tweets so only valid texts and words remain.  Then, I define the number of max features as 2000 and use Tokenizer to vectorize and convert text into Sequences so the Network can deal with it as input."
      ]
    },
    {
      "metadata": {
        "id": "7hTnAcguKU8g",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "fbed4736-0947-4d29-b5fd-45e5430e2119",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1530236616902,
          "user_tz": -330,
          "elapsed": 2620,
          "user": {
            "displayName": "Abraar Syed",
            "photoUrl": "//lh5.googleusercontent.com/-ocaYyjrX3Z4/AAAAAAAAAAI/AAAAAAAAABQ/cPl8U19wmMg/s50-c-k-no/photo.jpg",
            "userId": "115904709554351828058"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "data = data[data.sentiment != \"Neutral\"]\n",
        "data['text'] = data['text'].apply(lambda x: x.lower())\n",
        "data['text'] = data['text'].apply((lambda x: re.sub('[^a-zA-z0-9\\s]','',x)))\n",
        "\n",
        "print(data[ data['sentiment'] == 'Positive'].size)\n",
        "print(data[ data['sentiment'] == 'Negative'].size)\n",
        "\n",
        "for idx,row in data.iterrows():\n",
        "    row[0] = row[0].replace('rt',' ')\n",
        "    \n",
        "max_fatures = 2000\n",
        "tokenizer = Tokenizer(nb_words=max_fatures, split=' ')\n",
        "tokenizer.fit_on_texts(data['text'].values)\n",
        "X = tokenizer.texts_to_sequences(data['text'].values)\n",
        "X = pad_sequences(X)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4472\n",
            "16986\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/preprocessing/text.py:172: UserWarning: The `nb_words` argument in `Tokenizer` has been renamed `num_words`.\n",
            "  warnings.warn('The `nb_words` argument in `Tokenizer` '\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "dUdhx24bKU8k",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Next, I compose the LSTM Network. Note that **embed_dim**, **lstm_out**, **batch_size**, **droupout_x** variables are hyperparameters, their values are somehow intuitive, can be and must be played with in order to achieve good results. Please also note that I am using softmax as activation function. The reason is that our Network is using categorical crossentropy, and softmax is just the right activation method for that."
      ]
    },
    {
      "metadata": {
        "id": "Wnhe3GlKKU8l",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 343
        },
        "outputId": "0c077258-c2e2-446a-86fa-a070b9a52003",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1530236618340,
          "user_tz": -330,
          "elapsed": 1401,
          "user": {
            "displayName": "Abraar Syed",
            "photoUrl": "//lh5.googleusercontent.com/-ocaYyjrX3Z4/AAAAAAAAAAI/AAAAAAAAABQ/cPl8U19wmMg/s50-c-k-no/photo.jpg",
            "userId": "115904709554351828058"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "embed_dim = 128\n",
        "lstm_out = 196\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_fatures, embed_dim,input_length = X.shape[1], dropout=0.2))\n",
        "model.add(LSTM(lstm_out, dropout_U=0.2, dropout_W=0.2))\n",
        "model.add(Dense(2,activation='softmax'))\n",
        "model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
        "print(model.summary())"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: UserWarning: The `dropout` argument is no longer support in `Embedding`. You can apply a `keras.layers.SpatialDropout1D` layer right after the `Embedding` layer to get the same behavior.\n",
            "  \"\"\"\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(196, dropout=0.2, recurrent_dropout=0.2)`\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (None, 28, 128)           256000    \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 196)               254800    \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 2)                 394       \n",
            "=================================================================\n",
            "Total params: 511,194\n",
            "Trainable params: 511,194\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "AfGj5ZaYKU8o",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Hereby I declare the train and test dataset."
      ]
    },
    {
      "metadata": {
        "id": "giWPfCmQKU8o",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "d7548874-b72a-4bbd-b613-405ee42dd011",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1530236619571,
          "user_tz": -330,
          "elapsed": 1182,
          "user": {
            "displayName": "Abraar Syed",
            "photoUrl": "//lh5.googleusercontent.com/-ocaYyjrX3Z4/AAAAAAAAAAI/AAAAAAAAABQ/cPl8U19wmMg/s50-c-k-no/photo.jpg",
            "userId": "115904709554351828058"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "Y = pd.get_dummies(data['sentiment']).values\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.33, random_state = 42)\n",
        "print(X_train.shape,Y_train.shape)\n",
        "print(X_test.shape,Y_test.shape)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(7188, 28) (7188, 2)\n",
            "(3541, 28) (3541, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "AKV_6eDMKU8s",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Here we train the Network. We should run much more than 7 epoch, but I would have to wait forever for kaggle, so it is 7 for now."
      ]
    },
    {
      "metadata": {
        "id": "kVHuCIDRKU8s",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "0816bb10-14bb-4255-d1d6-46c6bf91194a",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1530236748187,
          "user_tz": -330,
          "elapsed": 127531,
          "user": {
            "displayName": "Abraar Syed",
            "photoUrl": "//lh5.googleusercontent.com/-ocaYyjrX3Z4/AAAAAAAAAAI/AAAAAAAAABQ/cPl8U19wmMg/s50-c-k-no/photo.jpg",
            "userId": "115904709554351828058"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "model.fit(X_train, Y_train, nb_epoch = 7, batch_size=batch_size, verbose = 2)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/models.py:981: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
            "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/7\n",
            " - 19s - loss: 0.4240 - acc: 0.8218\n",
            "Epoch 2/7\n",
            " - 18s - loss: 0.3022 - acc: 0.8730\n",
            "Epoch 3/7\n",
            " - 18s - loss: 0.2678 - acc: 0.8922\n",
            "Epoch 4/7\n",
            " - 18s - loss: 0.2306 - acc: 0.9065\n",
            "Epoch 5/7\n",
            " - 18s - loss: 0.1946 - acc: 0.9183\n",
            "Epoch 6/7\n",
            " - 18s - loss: 0.1733 - acc: 0.9299\n",
            "Epoch 7/7\n",
            " - 18s - loss: 0.1527 - acc: 0.9375\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7feda28fefd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "metadata": {
        "id": "5p50hGu8KU8w",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Extracting a validation set, and measuring score and accuracy."
      ]
    },
    {
      "metadata": {
        "id": "VXEBzNrMKU8x",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "e190cd65-3e16-45bc-b4aa-6f5772372948",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1530236750650,
          "user_tz": -330,
          "elapsed": 2427,
          "user": {
            "displayName": "Abraar Syed",
            "photoUrl": "//lh5.googleusercontent.com/-ocaYyjrX3Z4/AAAAAAAAAAI/AAAAAAAAABQ/cPl8U19wmMg/s50-c-k-no/photo.jpg",
            "userId": "115904709554351828058"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "validation_size = 1500\n",
        "\n",
        "X_validate = X_test[-validation_size:]\n",
        "Y_validate = Y_test[-validation_size:]\n",
        "X_test = X_test[:-validation_size]\n",
        "Y_test = Y_test[:-validation_size]\n",
        "score,acc = model.evaluate(X_test, Y_test, verbose = 2, batch_size = batch_size)\n",
        "print(\"score: %.2f\" % (score))\n",
        "print(\"acc: %.2f\" % (acc))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "score: 0.55\n",
            "acc: 0.83\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "THb93IAxKU81",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Finally measuring the number of correct guesses.  It is clear that finding negative tweets goes very well for the Network but deciding whether is positive is not really. My educated guess here is that the positive training set is dramatically smaller than the negative, hence the \"bad\" results for positive tweets."
      ]
    },
    {
      "metadata": {
        "id": "H6Yu7xsjKU84",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "2fdd54d5-ee50-4e09-b4b8-595adb852275",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1530236757382,
          "user_tz": -330,
          "elapsed": 6686,
          "user": {
            "displayName": "Abraar Syed",
            "photoUrl": "//lh5.googleusercontent.com/-ocaYyjrX3Z4/AAAAAAAAAAI/AAAAAAAAABQ/cPl8U19wmMg/s50-c-k-no/photo.jpg",
            "userId": "115904709554351828058"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "pos_cnt, neg_cnt, pos_correct, neg_correct = 0, 0, 0, 0\n",
        "for x in range(len(X_validate)):\n",
        "    \n",
        "    result = model.predict(X_validate[x].reshape(1,X_test.shape[1]),batch_size=1,verbose = 2)[0]\n",
        "   \n",
        "    if np.argmax(result) == np.argmax(Y_validate[x]):\n",
        "        if np.argmax(Y_validate[x]) == 0:\n",
        "            neg_correct += 1\n",
        "        else:\n",
        "            pos_correct += 1\n",
        "       \n",
        "    if np.argmax(Y_validate[x]) == 0:\n",
        "        neg_cnt += 1\n",
        "    else:\n",
        "        pos_cnt += 1\n",
        "\n",
        "\n",
        "\n",
        "print(\"pos_acc\", pos_correct/pos_cnt*100, \"%\")\n",
        "print(\"neg_acc\", neg_correct/neg_cnt*100, \"%\")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "pos_acc 52.103559870550164 %\n",
            "neg_acc 92.52728799328295 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "r_ecOFECKU87",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "775a8653-aa29-49a7-95a6-a10818027767",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1530236758401,
          "user_tz": -330,
          "elapsed": 989,
          "user": {
            "displayName": "Abraar Syed",
            "photoUrl": "//lh5.googleusercontent.com/-ocaYyjrX3Z4/AAAAAAAAAAI/AAAAAAAAABQ/cPl8U19wmMg/s50-c-k-no/photo.jpg",
            "userId": "115904709554351828058"
          }
        }
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 16,
      "outputs": []
    }
  ]
}